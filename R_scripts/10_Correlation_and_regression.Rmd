---
title: "Correlation and Regression"
author: "John Shorter"
date: "Mar/13/2024"
output:
  html_document:
    toc: yes  
  html_notebook:   
    number_sections: yes    
    toc: yes  
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

So far we went over one-way ANOVA, randomized block design, multifactorial design,
repeated measures and split field design. A common feature of these designs is that
the independent variables are always factors, and the dependent variable is always numeric.
To analyze these experiments, we use ANOVA followed by Tukey tests.

Now we're going to switch gear to a very different kind of experiment.
In this case, both the independent and dependent variables are numeric.
This is where correlation and regression become useful.

In this unit, we'll cover

1.  What is correlation and how to interpret them?
2.  What is regression?
3.  How to perform a regression analysis?

# Load package

```{r}
library(ggplot2) 
library(tidyr)
library(dplyr)
library(readr)
library(readxl)
library(RColorBrewer)
library(viridis)
```

# What is correlation, what it means, and what it does not mean.

Correlation describes as one variable changes, the other variable changes in a consistent direction.

To visualize that, we'll use income and child mortality rate across countries as an example.

```{r}
child_mortality <- read_csv("/work/Data/Rmodule_data/child_mortality_0_5_year_olds_dying_per_1000_born.csv", col_types = cols()) 
babies_per_woman <- read_csv("/work/Data/Rmodule_data/children_per_woman_total_fertility.csv", col_types = cols()) 
income <- read_csv("/work/Data/Rmodule_data/income_per_person_gdppercapita_ppp_inflation_adjusted.csv", col_types = cols())  
```

These are two datasets downloaded from [the Gapminder foundation](https://www.gapminder.org/data/).
The Gapminder foundation has datasets on life expectancy, economy, education, population across countries and years.
The goal is to remind us not only the "gaps" between developed and developing worlds,
but also the amazing continuous improvements of quality of life through time.

1.  Child mortality (0 - 5 year old) dying per 1000 born.
2.  Median income per person.
3.  Births per woman.

These were recorded from year 1800 and projected all the way to 2100.

```{r}
head(child_mortality)
head(income)
head(babies_per_woman)
```

These tables are not in tidy format. The years spread out across 200+ columns.
Let's make them into the tidy format first, and then we'll join them by matching the country and year columns.

```{r}
child_mortality_tidy <- child_mortality %>% 
  pivot_longer(names_to = "year", values_to = "death_per_1000", cols = c(2:302)) 

income_tidy <- income %>% 
  pivot_longer(names_to = "year", values_to = "income", cols = c(2:242)) 

babies_per_woman_tidy <- babies_per_woman %>% 
  pivot_longer(names_to = "year", values_to = "birth", cols = c(2:302)) 

mortality_and_income_and_birth <- child_mortality_tidy %>% 
  inner_join(income_tidy, by = c("country", "year")) %>%
  inner_join(babies_per_woman_tidy, by = c("country","year")) %>% 
  mutate(log10_income = log10(income))

head(mortality_and_income_and_birth)
```

(FYI: income has a huge range and not normally distributed. The best way is to do a log transformation on income first.)
There are a lot of data. For the sake of this discussion, we'll just look at year 1945 (when WWII ended) for now.

```{r}
data_1945 <- mortality_and_income_and_birth %>% 
  filter(year == 1945)
```

I'm going to plot log10(income) on x axis and child mortality on y axis.

```{r}
data_1945 %>% 
  ggplot(aes(x = log10_income, y = death_per_1000)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2), 
        text = element_text(size = 12, color = "black", face = "bold"), 
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

You can call this kind of plot a scatter plot, i.e., the dots are scattered.
As you can see, across countries, as income increases, child mortality decreases.
With this, we can verbally say that "income and child mortality are negatively correlated (or anti-correlated)".

But what is the statistics to quantify that?
We will use the correlation test.

The correlation test, like any statistical methods, have a few assumptions.

1.  Each observation is independent of each other. The measurement of one does not affect the other, and no repeated measures.
2.  Normality: the errors are normally distributed.
3.  Equal variance across the range of values
4.  No outliers

The above four assumptions really look like the assumptions of ANOVA. And they essentially are.
However, there are two more.

5.  Related pairs: each x value has one and only one corresponding y value.
6.  Linearity: There is a linear relation between the two variable.

It turns out the easiest way to check these assumptions is to just stare at the scatter plot.

Does it look like a linear relation? - Yeah.
Does it look like there is more or less an even spread of dots along the range of values? - More or less okay.
Are there any outliers (dots that are all the way out there)? - There is quite a spread, but I don't think there are outliers.

## The correlation test

To do a correlation test, use the `cor.test()` function.

```{r}
cor.test(data_1945$death_per_1000, data_1945$log10_income)
```

In the most applied sense, the two important numbers are "cor" and p-value.
The "cor" is the correlation coefficient. In publications it often times is notated the by lowercase letter "r".
It ranges from +1 to -1.
A value of +1 means the two variables are perfectly correlated.
= Increase in one variable leads to a perfect, consistent increase in the other variable.
A value of -1 means the two variables are perfectly negatively correlated.
= Increase in one variable leads to a perfect, consistent decrease in the other variable.

A value of 0 means there is no correlation between the two variables,
and the null hypothesis of r is 0.

The p value is the probability of getting a r more extreme than observed r,
given the number of degrees of freedom, and if r were to be 0.

So what does the this correlation test tell us?
r = -0.62, so we have a moderate correlation. It's not surprising, as there is a quite a spread across the line.
The p-value is < 2.2e-16, which means if r were to be 0,
the probability of finding r < -0.62 and more extreme is less than 2.2e-16. 
So we should reject the null hypothesis.
So log10(income) and child mortality are definitely anti-correlated.

**Note that the p value does not indicate how strong the correlation is.** 
This is very important, and so many people confuse p value with the strength of the correlation. 
A low p value does not always mean a strong correlation.
Only when both |r| is near 1, and the p value is low do you have a strong correlation.

## Be VERY CAREFUL when you interpret a correlation.

1.  If two variables are correlated, it DOES NOT mean one cause the other.
2.  Correlation is non-directional. It does not say which variable is independent, and which is dependent. So cor.test(x, y) gives the same result as cor.test(y, x).

## Non-linear correlation

Can I do a correlation when the trend is non-linear?
Yes, but you have to do ordinal correlation instead.
You will correlate the rank order of the variables, instead of the actual numbers.
Let's do an example.

Now say we want to correlate child mortality and income (not log10).

Let's plot this dataset to check assumptions first

```{r}
data_1945 %>% 
  ggplot(aes(x = income, y = death_per_1000)) +
  geom_point(alpha = 0.8) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

Now you can see the trend is clearly non-linear. As income increases, mortality does not linearly decrease.
There is a workaround for this. We'll use the Spearman's method for ordinal correlation.
In the cor.test() function, we will add `method = "s"` argument to
specify we are using Spearman's method for correlating rank orders.

```{r}
cor.test(data_1945$death_per_1000, data_1945$income, method = "s")
```

What R dose here is it ranks the numbers in both variables first, then it correlates the ranks.
rho is the Spearman's version of `cor`. You would interpret rho and p-value the same way as you would for a common correlation.
We actually get very comparable results here.

# Simple linear regression

Let's look at mortality vs log10(income) again

```{r}
data_1945 %>% 
  ggplot(aes(x = log10_income, y = death_per_1000)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2), 
        text = element_text(size = 12, color = "black", face = "bold"), 
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

Now we know child mortality and log10(income) are correlated,
which means I can predict mortality with a given log10(income), and vice versa.
How do I do that?

To achieve that, we need to make a regression model.
A regression model is a linear model that describe a equation relating the two variables of interest.
In a linear regression, the relation will be y = ax + b,
where "a" is the slope and "b" is the intercept.
By performing a simple linear regression, you will find the values of slope and intercept.

Linear regression, has the same assumptions as correlation. Again, the best way to check is to stare at the scatter plot.

## Setting up a linear regression

To set up a linear regression, we'll use the `lm()` function

```{r}
model_1945 <- lm(death_per_1000 ~ log10(income), data = data_1945) 
```

Easy.
In this case, we probably want to do `mortality ~ log10(income)` rather than `log10(income) ~ mortality`.
This is under the assumption that the lack of income is the driving factor for higher child mortality.
Similarly, in actual experiments, say we are looking at the rate of an enzyme across different substrate concentrations,
we should do `rate ~ conc`, because the concentration is the manipulative variable, the rate is the response variable.

We can check the assumptions of linear regression the way we check assumptions of ANOVA.
Normality?

```{r}
plot(model_1945, which = 2)
```

Pretty normal

Equal variance?

```{r}
plot(model_1945, which = c(1, 3))
```

It looks rather okay to me. There might be a little less variance at the higher end, but it's not too bad.

## Interpret a linear regression

```{r}
summary(model_1945)
```

### The equation

The `summary()` function allows you to interpret your regression model.
The Coefficients are the intercept and slope.
Looks like we have a intercept = 893, and slope = -191.81.
You can look at the their t values as well.
Again the null hypothesis is t = 0. The further the observed t is from 0, the less likely the null hypothesis is correct.
So your equation will be: mortality = -191.81 * log10(income) + 893

The unit of child mortality is death per 1000 births. And note that income is in units of log10.
This equation tells us that, when income increases 10 fold (+1 unit in log10), child mortality on average decrease 191 per 1000 births.

### The goodness of fit

An important result of a linear regression is R squared.
R squared is the measurement of goodness-of-fit.
It is the fraction of variation explained by the model.
It ranges from 0 to 1.
A value of 0 means 0% of the variance is explained by the model, meaning poor fit.
A value of 1 means 100% of the variance is explained by the model, meaning perfect fit.

A related concept is adjusted R^2. Adjusted R^2 corrects for the number of parameters in the model.
In this example, we have two parameters: slope for log10(income), and intercept.
Adding more parameters to the model always increases R^2, because there is always variation can be explained by chance.
Adjusted R^2 corrects for that. The more parameters in the model, the lower the adjusted R^2.
We will explain adjusted R^2 more in depth when we talk out polynomial curve fitting.
The null hypotheses of R^2 and adjusted R^2 are 0.

In this case our R^2 is 0.38, which means 38% of the variances in child mortality can be explained by log10(income).
Of course this is far from 100%, as we know child mortality is affected by many other socioeconomic factors.

### Visualize your model

You might want to plot out your equation and see how well it fits the data.

1.  First find the range of the predictor. In this case the predictor is log10(income).
2.  Go from the lower end to the higher end of the predictor, one small step at a time (e.g. 0.1 increment at a time).
3.  Calculate the predicted values using the equation found by the regression model

```{r}
fitted_mortality <- data.frame(
  "log10_income" = seq(min(data_1945$log10_income), max(data_1945$log10_income), by = 0.1)
) %>%
  mutate(death_per_1000 = -191.81 * log10_income + 893 )  
```

```{r}
data_1945 %>% 
  ggplot(aes(x = log10_income, y = death_per_1000)) +
  geom_point(alpha = 0.8) +
  geom_line(data = fitted_mortality, color = "indianred", size = 1.2) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

That looks pretty good.

### Predict intermediate values

Say, if a country has median income of 10,000 dollars a year, what would the child mortality be?
Well, we just plug it in!
log10(10000) gives us 4.

```{r}
-191.81 * 4 + 893
```

That gives us 126 deaths per 1000 births, which is 12.6%.

# Curve fitting using linear model

A limitation of the simple linear regression is that a linear relationship is required.
However, under certain circumstances, you can also fit curves using linear model.

You can fit curves using linear model when the underlying mathematical relationship is given.

To understand that, let's use the R built-in dataset Puromycin as an example.
In this experiment, we are looking at the rate of an enzyme across different substrate concentrations.
There are two states: drug treated or non-treated. We will only use the non-treated control data for now.
(Ref: Treloar, M. A. (1974), Effects of Puromycin on Galactosyltransferase in Golgi Membranes, M.Sc. Thesis, U. of Toronto.)

```{r}
Puromycin_ctrl <- Puromycin %>% 
  filter(state == "untreated")

head(Puromycin_ctrl)
```

Let's visualize the data first

```{r}
Puromycin_ctrl %>% 
  ggplot(aes(x = conc, y = rate)) +
  geom_point() +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        ) 
```

This is clearly not linear. However, the underlying mathematical expression of enzyme kinetics is known:
V = Vmax(S/(S + K)),
where V is rate,
Vmax is the maximum rate,
S is the substrate concentration, and
K is a constant. When S = K, you get V = (1/2)*Vmax.
This is the classic Michaelis-Menten enzyme kinetics.

It turns out we can linearize the relationship using data transformation.
If we invert both sides of the equation, we'll get:

1/V = K/(Vamx * S) + 1/Vmax

This is also called the double reciprocal equation.

You can see that if we set 1/V as the dependent variable, and 1/S as the predictor,
we can get K/Vmax as the slope and 1/Vmax as the intercept.

So let's find the parameters now!

*Note*: this is not the mathematically "correct" method. This method finds the best fit for 1/V, instead of V itself. 
The more correct approach is to use a generalized linear model, which will not be covered in this introductory lesson.

```{r}
Puromycin_ctrl <- Puromycin_ctrl %>% 
  mutate(one_over_v = 1/rate) %>% 
  mutate(one_over_s = 1/conc)
```

```{r}
model_ctrl <- lm(one_over_v ~ one_over_s, data = Puromycin_ctrl) 
```

Before we proceed, let's check the assumptions first.

Normality?

```{r}
plot(model_ctrl, which = 2)
```

Looks pretty ok.

Equal variance?

```{r}
plot(model_ctrl, which = c(1, 3))
```

We might have a problem. The variance is larger at the higher end of 1/S. But there is really nothing we can do here.

## Interpret the model

```{r}
summary(model_ctrl)
```

Looks like we have a equation of 1/V = 2.15e-4 * (1/S) + 6.972e-3

So Vmax = 1/6.972e-3, and K = 2.15e-4/6.972e-3

```{r}
1/6.972e-3 #Vmax
2.15e-4/6.972e-3 #K
```

So Vmax = 143, and K = 0.03

In addition, we have an R^2 = 0.89, which means 89% of the variation in the data are explain by the model. Pretty good.

## Visualize the the model

First let's visualize the model in double reciprocal scale

```{r}
fitted_rate <- data.frame(
  "one_over_s" = seq(min(Puromycin_ctrl$one_over_s), max(Puromycin_ctrl$one_over_s), by = 0.1)
) %>%
  mutate(one_over_v = 2.15e-4*(one_over_s) + 6.972e-3) %>% 
  mutate(conc = 1/one_over_s) %>% 
  mutate(rate = 1/one_over_v)
```

```{r}
Puromycin_ctrl %>% 
  ggplot(aes(x = one_over_s, y = one_over_v)) +
  geom_point() +
  geom_line(data = fitted_rate, size = 1.2, color = "indianred") +
  labs(x = "1/S",
       y = "1/V") +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        ) 
```

We can also visualize the model under the original scale

```{r}
Puromycin_ctrl %>% 
  ggplot(aes(x = conc, y = rate)) +
  geom_point() +
  geom_line(data = fitted_rate, size = 1.2, color = "indianred") +
  theme_minimal() +
   theme(axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

You might have noticed that although our model estimated Vmax to be 143,
in the actual data, the rate can go as high as > 150.
This is clearly not perfect, but nonetheless it fits the lower concentration data points very well.



Lets look at one last example of correlations with the Penguin data from the ANOVA lesson

# Load packages
```{r}
# install.packages("emmeans")
# install.packages("multcomp")
# install.packages("multcompView")
# install.packages("viridis")
# remotes::install_github("allisonhorst/palmerpenguins") #download the data from Github
```


```{r}
library(emmeans) #four new packages you might need to install them 
library(multcomp)
library(multcompView)
library(palmerpenguins)

library(ggplot2) 
library(tidyr)
library(dplyr)
library(readr)
library(readxl)
library(RColorBrewer)
library(viridis)
```

# Data
The palmerpenguins R package (Horst et al. 2020), which includes body size measurements collected from 2007 - 2009 for three species of Pygoscelis penguins that breed on islands throughout the Palmer Archipelago, Antarctica.
```{r}
head(penguins)
str(penguins)
penguins %>% 
  group_by(species) %>% 
  count()
```

Lets make a simple plot of two numeric variables, bill length and bill depth, of the data.

```{r}
penguins %>% 
ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(size = 2, alpha = 0.8) + 
  labs(title = "Bill Length vs. Bill Depth",
       x = "Bill Length (mm)",
       y = "Bill Depth (mm)") +
  theme(plot.title = element_text(size = 25, hjust = .5),
        axis.title = element_text(size = 12, face = "bold"))
```

Notice any interesting patterns with the data?
Now let's add a line through our points with 'geom_smooth'

```{r}
ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(size = 2, alpha = 0.8) + 
  labs(title = "Bill Length vs. Bill Depth",
       x = "Bill Length (mm)",
       y = "Bill Depth (mm)") +
  theme(plot.title = element_text(size = 25, hjust = .5),
        axis.title = element_text(size = 12, face = "bold"))+
geom_smooth(method = "lm", se = F)
```

It seems straight forward, that as the bill depth decreases, the bill length gets longer.

But wait! We are combining data from 3 different species of penguins. What does it look like if we just color the 3 species? 

```{r}
ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + #adding color by species
  geom_point(size = 2, alpha = 0.8) + 
  scale_colour_manual(values = c("steelblue3","red3","purple4")) + #making nicer colors
  labs(title = "Bill Length vs. Bill Depth",
       x = "Bill Length (mm)",
       y = "Bill Depth (mm)") +
  theme(plot.title = element_text(size = 25, hjust = .5),
        axis.title = element_text(size = 12, face = "bold"))
```

Now what do you think about the relationship between bill depth and bill length?

Lets add in a smooth line!

```{r}
ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point(size = 2, alpha = 0.8) + 
  scale_colour_manual(values = c("steelblue3","red3","purple4")) +
  labs(title = "Bill Length vs. Bill Depth",
       x = "Bill Length (mm)",
       y = "Bill Depth (mm)") +
  theme(plot.title = element_text(size = 25, hjust = .5),
        axis.title = element_text(size = 12, face = "bold"))+
geom_smooth(method = "lm", se = F)
```

The above is an example of Simpson's Paradox.
Simpson’s Paradox is a data phenomenon in which a trend observed between variables is reversed when data are pooled, omitting a meaningful variable. When penguin species is omitted, bill length and depth appear negatively correlated overall. The trend is reversed when species is included, revealing an obviously positive correlation between bill length and bill depth within species.


# Exercise one

Now you have learned how to perform correlation test and how to perform linear regression.
Let's practice that!

We'll practice correlation test first. We'll use the income and child mortality data again.
This time, we'll use the data from year 2015.

```{r}
data_2015 <- mortality_and_income_and_birth %>% 
  filter(year == 2015)

head(data_2015)
```

Visualize the relation between birth and child mortality across countries in year 2015.
Make your plot here:

```{r}
 
```

Is there a correlation between births per woman and child mortality? Do a correlation test.

```{r}
 
```

What does the correlation coefficient tell you?

Between income vs mortality in 1945 and birth vs mortality in 2015, which has a stronger correlation?

# Exercise two

Now let's practice linear regression. We'll use the Puromycin data again.
However, we will use the drug treated data this time.
Again, we are looking at the rate of an enzyme across substrate concentrations,
but this time the enzyme is treated with the drug puromycin.

```{r}
Puromycin_treat <- Puromycin %>% 
  filter(state == "treated") %>% 
  mutate(one_over_v = 1/rate) %>% 
  mutate(one_over_s = 1/conc)

head(Puromycin_treat)
```

## Set up the model and interpret the model

What is the intercept and slope?
What is the R^2 and what does it mean?

Calculate K and Vmax from the coefficients.
How does puromycin affect the K and Vmax of this enzyme?

## Visualize the model under the double reciprocal scale and the original scale

Make your plots here:
