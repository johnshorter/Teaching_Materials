---
title: "Analyses of proportions, contingency tables and enrichment"
author: "John Shorter"
date: "Mar/20/2024"
output:
  html_document:
    toc: yes  
  html_notebook:   
    number_sections: yes    
    toc: yes  
    toc_float: yes 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2) 
library(tidyr)
library(dplyr)
library(readr)
library(readxl)
library(RColorBrewer)
library(viridis)
```

# Introduction

We spent quite a lot of time on ANOVA and regression, so in this unit, we will do something different. 
This unit, we will go over some analyses regarding proportions, classifications and enrichment. 
Hopefully by the end of the lesson, you will see that these are closely related analyses.

Let's start with a couple examples. 
Say you self a plant of genotype Aa. In the F1 offspring, you expect to get 1/4 AA, 1/2 Aa and 1/4 aa. 
This is the classical Mendelian genetics. 
In a real experiment, how do you test if your observed data fit your expectation?

Here is another example. 
Consider a hypothetical genetic disease. You suspect that the risk of getting the disease is associated with biological sex. 
How would you test your hypothesis?

In this lesson, you will learn to choose the appropriate statistical tests for each question, as well as how to interpret the results of these statical tests.

# Test of proportions - Chi-squared goodness of fit test

First let's go over how to test if given observations fit expected proportions. 
In this case, you will need to use the Chi-squared test for goodness of fit.

Before we proceed, let's go over the assumptions of Chi-square goodness of fit test.

1.  You have one categorical variable. This is analogous to a factor for ANOVA. 
    A categorical variable can be yes/no, male/female, easy/medium/hard, A/B/C/D/E, and Wild-type/mutant. 
    Basically anything discrete variable.

2.  The groups in categorical variable are mutually exclusive. This is self-explanatory. 
    For example, an observation can only be "yes" or "no", but not both.

3.  Lastly, like any other statistical test, you should have independent observations, 
    that is the measurement of one observation does not affect the other, and there are no repeated measures.

Now let's do an example. Say I self a heterozygous plant of genotype Aa. 
I expect 1/4 AA, 1/2 Aa and 1/4 aa. 
And in this hypothetical experiment, among 96 F1 offspring, 
I have 32 AA, 54 Aa, and 10 aa.

You can see that 32 is quite a bit more than 1/4 and 54 is quite a bit more than 1/2, 
which means 10 is is quite a bit less than 1/4. 
How do test if the observed frequencies fit the expected?

You will do a Chi-square test for goodness of fit.

```{r}
chisq.test(x = c(32, 54, 10), 
           p = c(0.25, 0.5, 0.25))
```

In the `chisq.test()` command, you provide two arguments. 
First is the observed frequencies. In this case a vector of three observations, 32, 54 and 10. 
The second argument is the expected proportions, which is 0.25, 0.5 and 0.25.

In this case, we have Chi-squared = 11.583, and p value = 0.003. 
The p value is computed from the Chi-squared distribution. 
In this case, we have 2 degrees of freedom, which is number of categories - 1. 
We have 3 categories, AA, Aa and aa, so df = 3 - 1 = 2.

```{r}
#don't worry about this chunck. It simulates a Chi-squared distribution of df = 2 and n = 10^4.
#The vertical line is x = 11, our observed Chi-squared in this experiment 
set.seed(666)
rchisq(n = 10^4, df = 2) %>% 
  as.data.frame() %>% 
  ggplot(aes(x = .)) +
  stat_density(size = 1.2, alpha = 0.8) +
  geom_vline(xintercept = 11) +
  theme_classic()
```

The null hypothesis of Chi-squared is 0. A Chi-squared value of 0 would mean observed data fit expected proportions perfectly. 
The p value is computed as the right hand side area under the curve when x  > Chi-squared, which means 
p value is the probability of getting a Chi-squared value equal or larger than observed, if the null hypothesis were true. 
The larger the Chi-squared, the less likely the null hypothesis is true. 
In this case the p value is very small, so the observed data did NOT fit the expected observations.

For example, if you put in 24, 48 and 24, which are the expected frequencies in a n = 96 experiment, you will get Chi-squared = 0 and p = 1.

```{r}
chisq.test(x = c(24, 48, 24), p = c(0.25, 0.5, 0.25))
```

So what do our results (p = 0.003) mean? 
It means that there is more AA and Aa than expected, and less aa than expected. 
In a real world example, this could mean a lethality in the aa genotype, where aa seeds were aborted.

You noticed that here I used 96 plants, which was quite a lot. 
What will happen if there were less plants? 
Say in a hypothetical experiment, I only have 20 F1 offspring from the selfing the Aa parent. 
And I got 7 AA, 11 Aa, and 2 aa, which is similar in proportion as the n = 96 example above. 
If I put these numbers into a Chi-squared test, what will happen?

```{r}
chisq.test(x = c(7, 11, 2), p = c(0.25, 0.5, 0.25))
```

Now your p value is 0.26, which is larger than 0.05 and you would fail to reject the null hypothesis. 
*Be VERY CAREFUL* when you interpret large p values! 
Note that a p  > 0.05 does not prove the null hypothesis. It simply fails to reject it. 
The true proportions may actual deviate from the expected, but you didn't have enough observation (power) to detect it. 
As a rule of thumb, to perform a valid Chi-squared goodness of fit test, you need many observations, ideally  > 100.

# Contingency tables - Chi-squared test and Fisher Exact test

Now let's switch gears to talk about a related analysis, which is contingency tables, 
which is an analysis to study classifications and association.

Say in a hypothetical transcriptome study, 30000 genes were detected as expressed. 
Among them, 18000 genes were upregulated by heat treatment, whereas 12000 genes were downregulated or unchanged. 
Meanwhile, 24000 genes were upregulated by UV, where as 6000 were downregulated or unchanged.

Among these genes, 15000 genes were both upregulated in UV and heat, 9000 up in only UV but not heat, 
3000 up in only heat and not UV, and 3000 not changed in both UV and heat.

The question here is: do UV-upregulated genes tend to also be heat upregulated? 
Or are UV-upregulated genes associated with heat-upregulation?

Numerically, you can say, we have 30000 genes, and half of them are shared by both UV-upregulation and heat-upregulation. 
So, yes. But what is the statistical way to quantify this?

Here is where contingency tables are useful:

|   headers   | UV-up | not UV-up | total |
|:-----------:|:-----:|:---------:|:-----:|
|   Heat-up   | 15000 |   3000    | 18000 |
| not heat up | 9000  |   3000    | 12000 |
|    total    | 24000 |   6000    | 30000 |

This is a contingency table. 
At the columns, you have UV-upregulated genes vs. not UV-upregulated genes. 
At the rows, you have heat-upregulated genes vs. not heat-upregulated genes. 
You have row totals and column totals. But most importantly, the row and column totals should add up to the grand total. 
In this case, everything should add up to 30000, which is all the expressed genes in this experiment. 
Note: when you set up a contingency table, it's very IMPORTANT that you check the row and column totals add up to the grand total. 
And yes, 15000 + 3000 + 9000 + 3000 = 30000. This is how you know you have set it up correctly.

Now you have the contingency table set up, you can feed it into a Chi-squared test. 
This is a slightly different Chi-squared test for goodness of fit, because we are not testing against given proportions. 
People call this Chi-squared test for independence, which means it tests for whether there is a relationship between two categories. 
In this case, whether there is a relationship between UV and heat upregulated genes.

```{r}
chisq.test(x = rbind(c(15000, 3000),
                     c(9000, 3000))) 
```

In this case, you feed the contingency table (without the "total" rows and columns) into the x argument of the `Chisq.test()` function

`rbind(c(15000, 3000), c(9000, 3000))` would be equivalent to `cbind(c(15000, 9000), c(3000, 3000))`, 
where `rbind()` is binding two vectors as rows, and `cbind()` is binding two vector as columns.

```{r}
chisq.test(x = cbind(c(15000, 9000),
                     c(3000, 3000))) 
```

So we have a Chi-squared of 311, which is huge. 
And not surprisingly, the p value is very small. 
This supports the hypothesis that genes upregulated by UV also tend to be upregulated by heat.

A drawback of Chi-squared test is you need a large number of observations, ideally  > 100. 
What could you do if you don't?

A equivalent test is the Fisher Exact test. 
It is a non-parametric test for contingency tables. 
It relies on permutation of the contingency table to compute the p value, instead of using the Chi-squared distribution. 
It is valid for both small and large sample sizes.

If we feed in our contingency table to the `fisher.test()`, it will give us comparable results.

```{r}
fisher.test(x = rbind(c(15000, 3000),
                     c(9000, 3000))) 
```

As expected, the p value is very small, again supporting that genes upregulated by UV also tend to be upregulated by heat. 
Fisher Exact test also returns a number called the odds ratio (OR) for 2x2 contingency tables. So in case it will be (15000/9000) / (3000/3000) = 1.67, which means 
it's 1.67 times more likely that a UV-upregulated genes is also heat-upregulated. 
Don't worry about why the Fisher estimate is not exactly the same as the arithmetic OR, 
and don't worry about how the confidence interval is computed. 
The null hypothesis of OR is 1, meaning there is equal chance than UV-regulated genes is heat-upregulated, or not. 
The farther away OR is from 1, the smaller the p value, and less likely the null hypothesis is true.

Let's do another example for Fisher exact test. 
In this hypothetical example, a person claims they can tell the difference between a can of coca cola and and a can of pepsi. 
Here is the contingency table:

|    header     | actual coke | actual pepsi | total |
|:-------------:|:-----------:|:------------:|:-----:|
| claimed coke  |      8      |      2       |  10   |
| claimed pepsi |      3      |      7       |  10   |
|     total     |     11      |      9       |  20   |

The columns are whether the drink was actually a cola or a pepsi. 
The rows are whether the person claims it was a cola or a pepsi.

Can this person actually tell the difference between a coke and a pepsi? 
Let's feed the contingency table into the `fisher.test()` function.

```{r}
fisher.test(x = rbind(c(8, 2),
                      c(3, 7))) 
```

Well, the p value is 0.07. Notice that although this person has a higher chance in getting it right. 
The odds ratio is 8, so they are 8 times more likely in getting things right than wrong. 
However, because the sample size is relatively small, the p value is not as significant as expected. 
In this case, it might be a good idea for a few more cans of drinks (more observations). 
Again, p  > 0.05 dose NOT prove the null hypothesis. It just might be not enough observations.

If you try to use Chi-squared test here

```{r}
chisq.test(x = rbind(c(8, 2),
                      c(3, 7)))
```

You get a warning "Chi-squared approximation may be incorrect". 
This is because in order to compute the p value, R has to use the Chi-squared distribution. 
And the Chi-squared distribution is only valid when there are a lot of observations. 
But either way the results from both tests are pretty comparable, and the conclusion from the tests would be: 
You need more observations.


# Enrichment and depletion

## Using contingency table

Oftentimes, you might be interested in whether a sample from a population is enriched or depleted for certain types of individuals. 
A common application is gene ontology enrichment. 
Every gene in the genome is annotated with a certain number of "gene ontology" terms, for example "cell cycle", "transcription", etc. 
Say I have 100 genes that are upregulated in the east side of the stem in the morning of a sunflower plant. 
The sunflower plant track the sun during the day from east to west, and re-orient to the east during the night. 
The reason it can do that is because during the day the east side of the stem grows faster, pushing the head towards the west. 
And during the night, the west side of the stem grows faster, pushing the head back to the east.

So in this hypothetical experiment, we have 100 genes upregulated in the east side of the stem in the morning. 
According to the idea of solar tracking, the east side grows more during the day. 
So you want to know is the east side of the stem enriched for genes related to growth?

To answer this question, there are a few other numbers you have to know. 
(I made up these numbers, just as an example.) 
Say in your experiment, you detected 12000 expressed genes, 
among which 2000 of them have the annotation of growth or cell elongation-related.

And you have a list of 100 upregulated genes from 12000 expressed genes in this morning-east stem sample, 
among which 40 of them have the annotation of growth or cell elongation-related.

There are actually two tests to approach this problem. 
The first one is using a contingency table. SO let's set up the contingency table.

Let's fill in this table:

|       header       | upregulated | not upregulated | total |
|:------------------:|:-----------:|:---------------:|:-----:|
|   growth-related   |             |                 |       |
| not growth-related |             |                 |       |
|       total        |             |                 |       |

We know we have 12000 expressed genes, so the grand total must add up to 12000. 
We know 100 are upregulated. 
We know 40 of them are both upregulated and growth related, which means 60 of them are upregulated and not growth related. 
This completes the first column.

We know we have 2000 genes total related to growth, but only 40 are upregulated. 
So that means 1960 are growth related but not upregulated. This completes the first row. 
Now we have one last spot left. 
In a contingency table, the non-total cells have to add up to the grand total. 
So number of genes that are not upregulated and not growth related is: 
12000 - 40 - 60 - 1960 = 9940.

Here is the table:

|       header       | upregulated | not upregulated | total |
|:------------------:|:-----------:|:---------------:|:-----:|
|   growth-related   |     40      |      1960       | 2000  |
| not growth-related |     60      |      9940       | 10000 |
|       total        |     100     |      11900      | 12000 |

We have the contingency table, so let's feed it to a Fisher Exact test.

```{r}
fisher.test(x = rbind(c(40, 1960),
                      c(60, 9940)))
```

Our odds ratio is (40/60)/(1960/9940) = 3.38, 
which means it's 3.38 times more likely to find a growth related gene upregulated in morning-east. 
The p-value is pretty small (3.3e-8), so we will reject the null hypothesis of OR = 1. 
And the conclusion is yes, growth related genes are enriched in east side of the stem in the morning.


## Using contingency table when our data is numeric

Let's run through a final example where we want to group together observations into bins for a categorical test. Let's use this built-in cars data set. 

```{r}
# Load the cars dataset
data(cars)

# plot the dataset
plot(cars)
```


We will now make them into bins, based on some predefined ideas on speed and distance. 

```{r}
# Categorize stopping distances into bins
# Let's say we divide stopping distances into three categories: short, medium, and long
bins <- c(0, 10, 20, max(cars$dist))
categories <- cut(cars$dist, breaks = bins, labels = c("Short", "Medium", "Long"))

#and the same for speed
bins2 <- c(0, 10, 17, max(cars$speed))
categories2 <- cut(cars$speed, breaks = bins2, labels = c("Slow", "Medium", "Fast"))

# Create a contingency table of speed and stopping distance categories
contingency_table <- table(categories2, categories)

#This is needed to make the heatmap
contingency_df <- as.data.frame.table(contingency_table)

# Plot heatmap using ggplot2
ggplot(contingency_df, aes(x = categories2, y = categories, fill = Freq)) +
  geom_tile(color = "black") +
  scale_fill_gradient(low = "white", high = "darkblue") +
  labs(title = "Contingency Table Heatmap",
       x = "Speed",
       y = "Stopping Distance",
       fill = "Frequency") +
  theme_minimal()
```

Let's see if there is a significant difference in our new groups

```{r}
# Perform fisher.test
fisher_result <- fisher.test(contingency_table)

# Print the result
print(fisher_result)
```


# Exercise

## Chi-squared goodness of fit test

You did two crosses. 
You crossed a Aa mother to a AA father, you got 30 Aa and 60 AA. 
You crossed a AA mother to a Aa father, you got 55 Aa and 50 AA. 
(The expected proportions should be 1:1.)

Is there distortion of segregation ratio in either of the crosses? 
If so, which parent is the defect associated with?

```{r}
# You will need to do two chisq.tests. put them here. 


```


Now let's look at the built in dataset diamonds, and see if there is a difference in the cut and color variables.

```{r}
# Load the diamonds dataset
data(diamonds)

# View the first few rows of the dataset
head(diamonds)

```


```{r}
# Plot the frequency of cut and color. I recommend using ggplot2 and 'geom_bar'


# Perform Chi-square test


# Print the result


```

