---
title: "Data transformations"
author: "John Shorter"
date: "March/6/2024"
output:
  html_document:
    toc: yes  
  html_notebook:   
    number_sections: yes    
    toc: yes  
    toc_float: yes 
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

We talked about how to set up a linear model, how to run ANOVA, how to run Tukey tests and how to interpret the results of ANOVA and Tukey tests.
We also talked about the assumptions of linear models:

1.  Independence;
2.  Normal distributions of errors
3.  Equal variance across groups

However, in real experiments, sometimes the errors are not normally distributed,
as well as variances may differ widely across treatments.

In this unit, we will cover what should we do when non-normally distributed errors and uneven variances occur.
We will cover two techniques:

1.  Data transformations
2.  Pairwise Wilcoxon rank sum tests

# Load packages

```{r}
library(emmeans)  
library(multcomp)
library(multcompView)
library(ggplot2) 
library(tidyr)
library(dplyr)
library(readr)
library(readxl)
library(RColorBrewer)
library(viridis)
```

# Data transformations

The idea of data transformations is that instead of comparing the response variable in its original scale,
a mathematical operation is done on the response variable to a transformed scale,
and then compare in the transformed scale.

For example, pH is a commonly used data transformation for acidity (concentration of H+) of solutions.
It is the -log10 value of H+ concentration in M, or mol/L.
Say pure water has [H+] = 10^-7 M, thus pH = 7.
And that's a data transformation with -log10() being the mathematical operation.
.

In addition to statistics, data transformation is commonly used to make communication easier and more intuitive.
It's easier to say pH of water is 7 then say [H+] of water is 10^-7 moles per liter.

We'll cover two commonly used transformations: log & logit, as well as when to use them.
We will cover log transformation first.

# log transformation

```{r}
C_elegans_DAPI <- read_excel("/work/Data/Rmodule_data/C_elegans_DAPI.xlsx")
head(C_elegans_DAPI)
```

This is an experiment conducted by MCB160L (Genetics Labs) students at UC Davis (Spring 2019).
In this experiment, there were two students per group.
Each group looked at number of DAPI bodies in C elegans oocyte during meiosis I.
They made as much as 6 observations per worm (A - F).
The worms were treated with 4 treatments:

1. L4440 = empty vector control (negative control);\
2 - 4. RNAi constructs targeting meiotic genes syp-2, rec-8 and zim-1, respectively

(FYI: In the empty vector control, the expectation was 6 DAPI bodies;
there should be more DAPI body in RNAi treated oocytes due to defective pairing or defective synapsis and all that.)

This is not a tidy data frame.
Let's make it a tidy data frame by gathering the observations,
so that each observation is in its own row.

```{r}
DAPI_tidy <- C_elegans_DAPI %>% 
  pivot_longer(names_to = "observation", values_to = "count", cols = c(A, B, C, D, E, `F`)) %>% 
  filter(is.na(count) == F)

head(DAPI_tidy)
```

Before we do any statistics, let's just visualize the data first.

```{r}
#I'm going to use my wisteria palette
wisteria <- c("grey65", "burlywood3", "khaki2", "plum1", "lightcyan2", "cornflowerblue", "slateblue3")
```

```{r}
DAPI_tidy %>% 
  ggplot(aes(x = treatment, y = count)) +
  geom_boxplot(aes(fill = treatment), alpha = 0.8, width = 0.7, outlier.shape = NA) +
  geom_point(aes(fill = treatment), alpha = 0.8, shape = 21, color = "black", size = 2,
             position = position_jitter(seed = 666, width = 0.1)) +
  scale_fill_manual(values = wisteria[c(1, 2, 5, 6)]) +
  labs(y = "No. DAPI bodies",
       x = "RNAi treatment") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

Looking at the graph, you may have noticed a problem.
The spread of each treatment differs widely.
rec-8 has much much more spread and any other treatments, whereas L4440 has no spread at all.

## When to use the log transformation?

We can actually take a look at the summary statistics and have a directly look at the variances.

```{r}
DAPI_summary <- DAPI_tidy %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(count),
            var = var(count),
            sd = sd(count),
            n = n()) 

DAPI_summary
```

As you can see, the variance of L4440 is 0, whereas that of rec-8 is 30.
As a rule of thumb, to do a linear model, you want all the variances to be within a 2-fold range.
(Ideally you want the variances to be exactly equal, which would never be the case in real life.)

Clearly, if you try to run linear model in the original scale, it will not meet the assumptions.

But to illustrate the principles,
let's set up a linear model in the original scale anyway and see how things will go wrong.

```{r}
model_DAPI <- lm(count ~ treatment, data = DAPI_tidy)
```

Let's check the assumptions.
Normality of errors?

```{r}
plot(model_DAPI, which = 2)
```

I think this looks pretty bad.
The errors at either extremes are VERY far from normally distributed.

Equal variances?

```{r}
plot(model_DAPI, which = c(1, 3))
```

These also look pretty bad.
Again we can clearly see that treatments with larger means have higher variances.

This is where a log transformation MIGHT be useful.
(It's not always useful. It doesn't always help, as you will see in a bit.)

Let's just take a look at the log curve first.

```{r}
data.frame(
   x = seq(0.1, 10, by = 0.1)
) %>% 
  mutate(y = log(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_hline(yintercept = 0, size = 1.2, color = "grey50") +
  geom_vline(xintercept = 0, size=  1.2, color = "grey50") +
  geom_line(size = 1.2) +
  theme_minimal() +
  theme(text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

As you can see, the log curve has the following properties:
The curve changes rapidly at small values and changes slowly at larger values.
When you log transform a variable, it spreads out the smaller values and squeezes the larger values.
This may help correct for the fact that treatments with larger means have larger variances.
Functions with similar properties (e.g. square root) can be used as well, but log is the most common.

Now let's do a data transformation by taking log(count) and pass it to the linear model.
(Note: you can't have 0 and negative values in `log()`.

If your data are all negative, then you can simply do `log(- data)`.

If your data contain both negative and positive values, you will need to shift the data to all positive.
For example, if the smallest value is -5, you will add 5 to everything, then proceed to adjusting for zeros - see below. 

If your data contain zeros, a trick you can do is find the smallest non-zero value and add half of that to all the zeros.

If all the non-zero data are large (say > 10), another trick is to +1 to every value.)

In this worm experiment, all values are > 0, so we'll just proceed.
And then we'll check the assumptions again.

```{r}
model_DAPI_log <- lm(log(count) ~ treatment, data = DAPI_tidy)
plot(model_DAPI_log, which = c(1, 2, 3))
```

Unfortunately, I don't think the log transformation helped much.
It might have helped a little bit, but the errors were still not normally distributed.

When things turn out like this, there are a few options.

1.  Give up.
2.  Do the Tukey tests anyway (either in transformed scale or original scale) and discuss how the results may not be reliable.
3.  Do non-parametric tests, such as the wilcoxon rank sum test.
4.  Try generalized linear models. These are a lot more complicated, and we won't cover them in this module.

# Wilcoxon rank sum tests

The term non-parametric tests refer to statistical tests that do not rely on pre-existing distributions.
As counter-examples, ANOVA and Tukey tests rely on the normal distribution, which makes them parametric tests.
Instead, non-parametric tests usually rely on somehow ranking or permutating the data to generate the P values.

Wilcoxon rank sum test, also called the Mann--Whitney U test, is a very commonly used non-parametric test.
It tests for differences in the central tendencies of two treatments.
(Note: Wilcoxon test actually tests the differences in median and not the means).

We won't cover the underlying mathematics of the Wilcoxon test, as this is an applied statistics lesson.
But essentially it relies on comparing the ranks across two treatments.

We have 4 treatments in our experiment, so we have a total of 4 choose 2 = 6 pairwise comparisons.
Fortunately, R has a built-in `pariwise.wilcox.test()` function.
It takes two arguments, x and g.
x is the variable to compare, in this case it will be the counts of DAPI bodies.
g is the groups or labels of the counts, in this case it will be which treatment the counts came from.

Inside the pairwise.wilcox.test(), you will use `DAPI_tidy$count` to call the count column from the DAPI_tidy data frame.
The `$` symbol calls a particular column from a data frame.
You will do the same for treatment as well.

```{r}
pairwise.wilcox.test(x = DAPI_tidy$count, g = DAPI_tidy$treatment)
```

Here are the results.
We have 6 comparisons, and the P values are shown in this table.
The null hypothesis of wilcoxon test is that the medians of two treatments are equal.
The P values are the probability of observing the given data if the medians were truly equal.
As you can see the P values are all very low, suggesting the median counts of DAPI bodies for all treatments are different from each other.

Before we move on, I just want to comment on multiple comparison corrections.
In this analysis, we have 6 comparisons.
Under an alpha < 0.05 cutoff, the total type I error rate will increase to about 0.05 * 6 = 0.3.
But the results of `pairwise.wilcox.test()` are actually already corrected for multiple comparisons by R;
This is indicated by the message "P value adjustment method: holm".
So we don't need to worry about multiple comparisons ourselves.

# logit transformation

Another very useful data transformation is the logit transformation.
Let's start with an example.

```{r}
C_elegans_lethality <- read_excel("/work/Data/Rmodule_data/C_elegans_lethality.xlsx")
head(C_elegans_lethality)
```

This is again an experiment conducted by MCB160L (Genetics Labs) students at UC Davis (Spring 2019).
In this experiment, there were two students per group.
Each group looked at proportion of dead embryos from worms treated with RNAi constructs.
The worms were treated with 4 treatments:

1. L4440 = empty vector control (negative control);\
2 - 4. RNAi constructs targeting meiotic genes syp-2, rec-8 and zim-1, respectively.

(FYI: In the empty vector control, the expectation was no dead embryos and perfectly fine;
there should be more dead embryos in RNAi treated worms.)

This is already a tidy data frame, so no further arrangement is required here.

Let's visualize the data first.

```{r}
C_elegans_lethality %>% 
  ggplot(aes(x = treatment, y = proportion)) +
  geom_boxplot(aes(fill = treatment), alpha = 0.8, width = 0.7, outlier.shape = NA) +
  geom_point(aes(fill = treatment), alpha = 0.8, shape = 21, color = "black", size = 2,
             position = position_jitter(seed = 666, width = 0.1)) +
  scale_fill_manual(values = wisteria[c(1, 2, 5, 6)]) +
  labs(y = "Proportion dead embryo",
       x = "RNAi treatment") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.line = element_line(size = 1.2),
        text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

Again Looking at the graph, we may have noticed a problem.
The spread of each treatment differs widely,
but in a way that is different from the DAPI count data.
In this case, the smaller and larger treatments have smaller spread,
while treatments in the mid-range has larger spreads.

## When to use a logit transformation?

We can actually take a look at the summary statistics and have a directly look at the variances.

```{r}
lethal_summary <- C_elegans_lethality %>% 
  group_by(treatment) %>% 
  summarise(mean = mean(proportion),
            var = var(proportion),
            sd = sd(proportion),
            n = n()) 

lethal_summary
```

The response variable is proportion, so it ranges from 0 to 1.
You may have noticed that treatments with means near 0 or 1 (L4440 and syp-2, respectively) have small variance,
while treatments near 0.5 have larger variances.

Again the variances differ for more than a 2-fold change.
So it will not meet the assumptions for linear model.
But let's run a linear model in the original scale anyway just to see how things will go wrong.

```{r}
model_lethal <- lm(proportion ~ treatment, data = C_elegans_lethality)
plot(model_lethal, which = c(1, 2, 3))
```

There are two problems here.

1.  Errors at either extremes are quite far from normally distributed.
2.  Treatments with mid-range means have higher variances.

This is where the logit transformation comes in handy.
It is defined as:\
logit(p) = log(p/(1 - p)), and p must be > 0 and < 1.
The logit function is specifically suited for proportion data. Sometimes p/(1 - p) is called "the odds".
For example, if p = 0.5, then the odds are 1 :
1. If p = 0.1, then the odds are 1 : 9.
If p = 0.01, then the odds are 1 : 99.

The reverse operation for logit() is logistic().
It is defined as:
logistic(x) = 1/(1 + exp(-x)).

Funny enough, R does not have built-in functions for logit() and logistic().
So we will write our own.

```{r}
logit <- function(p){log(
  p / (1-p)
)}

logistic <- function(x){
  1/(1 + exp(-x))
}
```

You don't have to learn how to write functions in R.
When you need to use logit and logistic in your future codes, you can come back here and copy and paste the above chunk.
But in case you're interested, the syntax is:

    name <- function(arguments){
       operations 
    }

You use the curve brackets `{}` to specify what operations you want the function to do.
For example, if I want a function to take the reciprocal of a number (f(x) = 1/x), it would be

```{r}
reciprocal <- function(x){
  1/x
}

#trying it out 
reciprocal(2)
```

Now let's look at the logit curve.

```{r}
data.frame(
  p = seq(0.001, 0.999, by = 0.001)
) %>% 
  mutate(y = logit(p)) %>% 
  ggplot(aes(x = p, y = y)) +
  geom_hline(yintercept = 0, size = 1.2, color = "grey50") +
  geom_vline(xintercept = 0, size=  1.2, color = "grey50") +
  geom_line(size = 1.2) +
  theme_minimal() +
  theme(text = element_text(size = 12, color = "black", face = "bold"),
        axis.text = element_text(size = 12, color = "black", face = "bold")
        )
```

The logit curve has a few properties:

1.  It is confined between p > 0 and p < 1;
2.  The curve changes rapidly near 0 and 1; and
3.  The curve changes slowly near 0.5.

When you logit transform a variable, it spreads out the values at either ends,
but squeezes the values in the mid-range.
This may help correct for that fact that treatments with means at the mid-range have higher variances.

## Adjustments before doing a logit transformation

Before you do a logit transformation, you need to check a couple things.

1.  Are my data all positive and between 0 and 1?
    If your data are larger than 1, what you can do is divide every value with the maximum value first.
    This scales down all the values as fractions of the maximum.

For example, say you are comparing the performances of students on an exam across teachers.
And you decided that you need to do a logit transformation.
Say the exam is out of 100 points.
You should divide each exam score by 100 first before doing the logit transformation.
This scales down the scores to fraction of the maximum possible scores.

Again, similar to log transformation,
If your data are all negative, then you can simply do logit(- data).
If your data contain both negative and positive values, you will need to shift the data to all positive.
For example, if the smallest data is -5, you will add 5 to everything, then divide by the maximum value,
then proceed to adjusting for zeros and ones - see below.

In this lethality experiment, the data are already proportions, so we don't need to scale down here.

2.  The second thing to check is are there any 0 and 1 in my data?
    You can't have 0 and 1 in your data because logit(0) and logit(1) will be -Inf and Inf, respectively.
    We do have 0 and 1 in our data.\
    The trick is:

    - 1. find the smallest non-zero value.

    - 2. add 1/2 of that to all zeros.

    - 3. subtract 1/2 of that to all ones.

```{r}
C_elegans_lethality %>% 
  filter(proportion > 0) %>% 
  summarise(min = min(proportion)) 
```

Looks like the smallest non-zero number is 0.0038.
We will add 0.0038/2 to all zeros, and subtract 0.0038/2 to all ones.

```{r}
lethal_offset <- C_elegans_lethality %>% 
  mutate(proportion_adjusted = case_when(
    proportion == 0 ~ 0.00384/2,
    proportion == 1 ~ 1 - 0.00384/2,
    TRUE ~ proportion   
  ))
```

The `TRUE ~ proportion` within `case_when()` here means for rest of the values that are not the above,
which are all values that are not 0 or 1, set the new proportions to be the original proportions, no change.

Now let's do the logit transformation and run a linear model.
We'll see if the logit transformation helps.

```{r}
model_lethal_logit <- lm(logit(proportion_adjusted) ~ treatment, data = lethal_offset)
plot(model_lethal_logit, which = c(1, 2, 3))
```

I think the normal qqplot has improved a lot.
So the logit transformation was definitely helping.
The variances across treatments are also a lot more even.
So we should proceed with the logit transformed data for linear model.

## How to interpret transformed data?

When you decided to proceed with data transformation,
it's a good idea to run the same analyses on both transformed and original data.
You can compare them and see how your conclusions may differ.

```{r}
anova(model_lethal)
anova(model_lethal_logit)
```

In terms of ANOVA, the results are pretty similar.
But the results for logit transformed data should be more valid, as the data meet the assumptions better.

Now let's estimate the means for the treatments and pull out the contrasts.

```{r}
estimate_lethal <- emmeans(model_lethal, pairwise ~ treatment)
estimate_lethal_logit <- emmeans(model_lethal_logit, pairwise ~ treatment)

estimate_lethal$contrasts
estimate_lethal_logit$contrasts
```

You can see that in the original scale,
the means of rec-8 and syp-2 are detected as not statistically different (P = 0.21 > 0.05).
But in the logit transformed scale, they are statically different (P = 0.015).
Again, the results for logit transformed data should be more valid, as the data meet the assumptions better.

Overall the results are pretty similar though.

**So if the differences are large enough, it practically doesn't change your conclusions.**

## How to report transformed data? 

For publications and presentations, you will need to report your results.
We'll report using the cld format.

```{r}
cld(estimate_lethal_logit$emmeans, Letters = letters)
```

Well, they are all statistically different from each other.
However, we have one more thing to do here.
Look at the emmean column.
These are the estimated means.
They are reported in the logit scale, not the original scale. The same for all other values.
To make sense of them, we need to de-transform them back to proportions using the `logistic()` function.
And you should report your estimated means and confidence intervals in the de-transformed scale.

```{r}
lethal_detransform <- cld(estimate_lethal_logit$emmeans, Letters = letters) %>% 
  as.data.frame() %>% 
  mutate(estimated_mean = logistic(emmean)) %>% 
  mutate(upper.CL_detrans = logistic(upper.CL)) %>% 
  mutate(lower.CL_detrans = logistic(lower.CL)) %>%
  inner_join(lethal_summary, by = "treatment") %>% 
  dplyr::select(treatment, mean, estimated_mean, lower.CL_detrans, upper.CL_detrans, .group, n)

lethal_detransform
```

For the sake of this discussion, I also joined the (arithmetic) means of the treatments to this table.
You can see that the means are different from their respective de-transformed estimated means.
This is very common for data transformations.
When you calculated the means in the transformed scale then de-transform,
the estimated means are different from the arithmetic means.
Both are correct though.
However, if you use a data transformation, you should report the de-transformed estimated means instead.

(FYI: if I remember the biology correctly, syp-2 RNAi has a catastrophic effect on meiosis I.
and should lead to ~100% dead embryos,
whereas as the empty vector control L4440 should have ~0% dead embryos.
Comparing the means and de-transformed estimated means,
I think the de-transformed estimated means better capture the biology.)

# Exercise

Now you have learned Wilcoxon tests and data transformation, it's time for you to practice!

## Wilcoxon rank sum test

Run pairwise wilcoxon tests on the C elegans lethality data and compare the results to that of the logit transformed Tukey tests.

```{r}
 
```

Are the your conclusions consistent between the two methods?

## Data transformation

Here are the results from an experiment.

```{r}
stem_data <- read_excel("/work/Data/Rmodule_data/LU-Matand - Stem Data.xlsx")
head(stem_data)
```

In this experiment, the researchers were looking at the effect of explant in daylily tissue culture.
(Data from Matand et al. 2020. BMC Plant Bio)


They took three types of explant (cut stem, inverted stem and split stem),
put those on agar plates and examined how well shoots regrew from them.
At the end of the experiment,
they counted the number of shoots from each tissue (Buds_Shoots column in the table).
There were other factors in this experiment, but let's ignore them for now.
They did this for a total of 19 daylily cult
ivars. For the sake of this exercise,
let's just use one of the cultivars - "Alias".

```{r}
Alias <- stem_data %>% 
  filter(Variety == "Alias")  

head(Alias)
```

### Visualize the data using Explant on x axis and Buds_Shoots on y axis.

Label the y axis "Number of regenerated shoots"

```{r}

```

Looking at the plot, do treatments have even spread?

### Calculate the summary statistics using group_by() and summarise() for Buds_Shoots

```{r}
 
```

Are the variances of treatments within a two-fold range?

### Linear model

Which data transformation would be helpful for this experiment?
Check the asumptions (normality & equal variance) of BOTH original AND transformed data.
Does the data transformation help?

```{r}
#set up linear models in this chunk

```

```{r}
#check assumptions in this chunk

```

### Interpretation

Run ANOVA and Tukey tests for BOTH original AND transformed data.
Did data transformation (or the lack of) change your conclusions?

```{r}

```

### Reporting

Report the estimated means and confidence intervals for the treatments in the cld format.
Don't forget to de-transform if you go with a data transformation.

```{r}

```
